# Learning under Extreme Data Scarcity: An Enhanced Hybrid CRNN with Calibration and Test-Time Augmentation for Multi-Class Lung CT Classification

[![Python 3.8+](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> **Maheswari V and Parveen Sultana H**
> School of Computer Science and Engineering, Vellore Institute of Technology, Vellore, Tamil Nadu 632014, India
> ğŸ“§ Correspondence: [hparveen.sultana@vit.ac.in](mailto:hparveen.sultana@vit.ac.in)

**An Enhanced Hybrid CRNN (EfficientNet-B0 + BiLSTM + Multi-Head Attention + Dual-Path Fusion) with temperature-scaled calibration and test-time augmentation for lung cancer subtype classification from CT images under extreme low-data constraints (~20 training samples).**

---

## Overview

This repository contains the complete implementation and reproducible experimental pipeline for our proposed **Enhanced Hybrid CRNN** architecture, which combines convolutional feature extraction with recurrent sequence modeling and attention mechanisms for lung CT classification. The model is designed to operate under a **5% coreset regime** (~20 training images selected via Farthest-Point Sampling), targeting realistic clinical scenarios where annotated medical data is scarce.

The framework incorporates **temperature-scaled calibration** for reliable confidence estimation and **test-time augmentation (TTA)** for robust inference. All models (proposed and baselines) are benchmarked under a **unified training protocol** (identical epoch budget, early stopping, and evaluation pipeline), with comprehensive ablation studies, calibration diagnostics, and statistical significance testing.

### Key Contributions

- **Dual-Path Fusion Architecture** â€” Combines a CNN classification path with a BiLSTM + Multi-Head Attention sequential path, fused via a learnable weight (Î± = 0.7), enabling the model to capture both spatial and sequential feature dependencies from CT feature maps.
- **Calibration and Confidence Reliability** â€” Applies post-hoc temperature scaling fitted on the validation set, reducing ECE from 0.1589 to 0.1190 across seeds, with reliability diagram analysis and adaptive ECE reporting for trustworthy clinical deployment.
- **Test-Time Augmentation** â€” Employs a 3-transform TTA ensemble (identity, horizontal flip, 5Â° rotation) that consistently improves accuracy and AUC over single-inference predictions.
- **Coreset Training with Farthest-Point Sampling** â€” Trains on only 5% of the data (~20 images) selected via class-stratified FPS in ResNet-18 embedding space, significantly outperforming random and stratified sampling baselines.
- **CLAHE-Based CT Preprocessing** â€” Applies Contrast-Limited Adaptive Histogram Equalization followed by morphological lung segmentation to enhance tissue contrast before feature extraction.
- **Rigorous Evaluation Protocol** â€” Multi-seed (3Ã—) Ã— multi-coreset (3Ã—) = 9 independent runs per model with bootstrap confidence intervals, Wilcoxon signed-rank tests, Cohen's d effect sizes, and calibration diagnostics.

---

## Architecture

```
Input CT Image (224Ã—224Ã—3)
        â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
   â”‚  CLAHE   â”‚  Contrast enhancement + morphological lung segmentation
   â”‚ Preproc  â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  EfficientNet-B0  â”‚  Pretrained backbone (ImageNet)
   â”‚  + SE Attention   â”‚  Squeeze-and-Excitation channel attention
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   â”‚         â”‚
   â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CNN  â”‚  â”‚ 1Ã—1 Conv Compress (â†’256)     â”‚
â”‚ Path â”‚  â”‚        â†“                     â”‚
â”‚ GAP  â”‚  â”‚ BiLSTM (128 hidden, bidir)   â”‚
â”‚  â†“   â”‚  â”‚        â†“                     â”‚
â”‚ FC   â”‚  â”‚ Multi-Head Attention (4 head)â”‚
â”‚  â†“   â”‚  â”‚        â†“                     â”‚
â”‚Logitsâ”‚  â”‚ Max+Mean Pool â†’ FC â†’ Logits  â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â”‚                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â–¼
   Dual-Path Fusion: Î±Â·RNN + (1-Î±)Â·CNN
           â”‚
     4-class output
```

**Classes:** Adenocarcinoma Â· Large Cell Carcinoma Â· Squamous Cell Carcinoma Â· Normal

---

## Results

### Main Comparison (9 runs: 3 seeds Ã— 3 coresets)

| Model | Accuracy | Macro-F1 | Macro-AUC | ECE |
|:------|:--------:|:--------:|:---------:|:---:|
| **Enhanced CRNN (Ours)** | **0.4523 Â± 0.0085** | **0.4637 Â± 0.0085** | **0.7302 Â± 0.0073** | 0.1589 Â± 0.0178 |
| Enhanced CRNN + TTA | 0.4557 Â± 0.0085 | 0.4671 Â± 0.0083 | 0.7357 Â± 0.0069 | 0.1501 Â± 0.0167 |
| Ensemble (Top-3) | 0.4105 Â± 0.0075 | 0.3983 Â± 0.0069 | 0.7428 Â± 0.0072 | 0.1345 Â± 0.0134 |
| ResNet-18 | 0.3842 Â± 0.0089 | 0.3654 Â± 0.0089 | 0.7073 Â± 0.0073 | 0.1923 Â± 0.0234 |
| ResNet-50 | 0.3567 Â± 0.0098 | 0.3363 Â± 0.0094 | 0.6801 Â± 0.0082 | 0.2078 Â± 0.0278 |
| DenseNet-121 | 0.3774 Â± 0.0085 | 0.3585 Â± 0.0088 | 0.6962 Â± 0.0076 | 0.1956 Â± 0.0256 |
| MobileNet-V3-Large | 0.3002 Â± 0.0082 | 0.2713 Â± 0.0081 | 0.5192 Â± 0.0085 | 0.0823 Â± 0.0189 |
| EfficientNet-B0 | 0.3447 Â± 0.0074 | 0.3147 Â± 0.0072 | 0.5510 Â± 0.0082 | 0.0567 Â± 0.0167 |
| ConvNeXt-Tiny | 0.3808 Â± 0.0089 | 0.3623 Â± 0.0084 | 0.7031 Â± 0.0072 | 0.1878 Â± 0.0223 |
| Random Baseline | 0.2542 | 0.2389 | 0.5000 | â€” |

The Enhanced CRNN achieves a **+17.7% relative improvement** in accuracy over the best baseline (ResNet-18) with statistical significance (p < 0.01, Wilcoxon signed-rank test) across all comparisons.

### Ablation Study

| Variant | Accuracy | Macro-F1 | Macro-AUC |
|:--------|:--------:|:--------:|:---------:|
| A: Backbone Only | 0.3509 Â± 0.0042 | 0.3217 Â± 0.0042 | 0.5545 Â± 0.0089 |
| B: + BiLSTM | 0.4085 Â± 0.0043 | 0.3962 Â± 0.0042 | 0.6923 Â± 0.0078 |
| C: + Attention | 0.3881 Â± 0.0042 | 0.3728 Â± 0.0043 | 0.6734 Â± 0.0082 |
| **D: Full CRNN** | **0.4509 Â± 0.0118** | **0.4623 Â± 0.0112** | **0.7312 Â± 0.0082** |

### Calibration & Test-Time Augmentation

| Configuration | Accuracy | Macro-F1 | ECE | Temperature |
|:--------------|:--------:|:--------:|:---:|:-----------:|
| Enhanced CRNN (base) | 0.4523 Â± 0.0085 | 0.4637 Â± 0.0085 | 0.1589 Â± 0.0178 | â€” |
| + Temperature Scaling | â€” | â€” | **0.1190 Â± 0.0037** | 1.16â€“1.35 |
| + Test-Time Augmentation | **0.4557 Â± 0.0085** | **0.4671 Â± 0.0083** | 0.1501 Â± 0.0167 | â€” |

Temperature scaling reduces ECE by ~25% with stable temperature values (range 1.16â€“1.35) across all 9 seed configurations, demonstrating reliable post-hoc calibration for clinical confidence estimation.

### Per-Class Performance (Enhanced CRNN)

| Class | Precision | Recall | F1-Score | Support |
|:------|:---------:|:------:|:--------:|--------:|
| Adenocarcinoma | 0.5135 | 0.4865 | 0.4997 | 74 |
| Large Cell Carcinoma | 0.4312 | 0.3919 | 0.4107 | 74 |
| Squamous Cell Carcinoma | 0.4067 | 0.4459 | 0.4254 | 74 |
| Normal | 0.5534 | 0.5068 | 0.5291 | 73 |

### Coreset Strategy Comparison

| Strategy | Accuracy | Macro-F1 |
|:---------|:--------:|:--------:|
| Random | 0.4068 Â± 0.0156 | 0.4156 Â± 0.0178 |
| Stratified | 0.4271 Â± 0.0112 | 0.4367 Â± 0.0134 |
| **FPS (Ours)** | **0.4542 Â± 0.0089** | **0.4645 Â± 0.0098** |

### Fusion Weight Sensitivity

| Î± (RNN / CNN) | Accuracy | Macro-F1 | Macro-AUC |
|:--------------:|:--------:|:--------:|:---------:|
| 0.5 / 0.5 | 0.4305 | 0.4412 | 0.7134 |
| 0.6 / 0.4 | 0.4441 | 0.4545 | 0.7212 |
| **0.7 / 0.3** | **0.4576** | **0.4689** | **0.7356** |
| 0.8 / 0.2 | 0.4508 | 0.4612 | 0.7289 |
| 0.9 / 0.1 | 0.4373 | 0.4478 | 0.7178 |

### Statistical Significance

All comparisons between the Enhanced CRNN and baselines are statistically significant at p < 0.01 (Wilcoxon signed-rank test, 9 paired observations). Cohen's d effect sizes range from 7.89 to 22.45, indicating large practical differences. Full statistical test results are available in `tables/statistical_tests.csv`.

---

## Dataset

**Source:** [`dorsar/lung-cancer`](https://huggingface.co/datasets/dorsar/lung-cancer) on Hugging Face

| Split | Samples |
|:------|--------:|
| Train | 400 |
| Validation | 72 |
| Test | 295 |
| **Coreset (5%)** | **~20** |

The dataset contains CT scan images across four classes of lung tissue. All images are preprocessed through CLAHE contrast enhancement and morphological segmentation before training.

---

## Repository Structure

```
â”œâ”€â”€ README.md
â”œâ”€â”€ crnn_v2_experiment.py          # Main experiment script (all phases)
â”œâ”€â”€ fig/                           # Generated figures
â”‚   â”œâ”€â”€ figS1_learning_curves_all.png
â”‚   â”œâ”€â”€ figS2_ablation.png
â”‚   â”œâ”€â”€ fusion_sensitivity.png
â”‚   â”œâ”€â”€ cm_*.png                   # Confusion matrices
â”‚   â”œâ”€â”€ roc_*.png                  # ROC curves
â”‚   â”œâ”€â”€ pr_*.png                   # Precision-Recall curves
â”‚   â”œâ”€â”€ reliability_*.png          # Reliability diagrams
â”‚   â””â”€â”€ loss_*.png                 # Per-model learning curves
â”œâ”€â”€ tables/                        # Generated CSV/JSON tables
â”‚   â”œâ”€â”€ aggregated_results.csv
â”‚   â”œâ”€â”€ statistical_tests.csv
â”‚   â”œâ”€â”€ ablation_results.csv
â”‚   â”œâ”€â”€ fusion_sensitivity.csv
â”‚   â”œâ”€â”€ coreset_comparison.csv
â”‚   â”œâ”€â”€ per_class_metrics.csv
â”‚   â”œâ”€â”€ model_parameters.csv
â”‚   â”œâ”€â”€ hyperparameters.json
â”‚   â”œâ”€â”€ environment.json
â”‚   â””â”€â”€ coreset_indices_*.json     # Per-run coreset indices
â””â”€â”€ ckpt/                          # Model checkpoints
```

---

## Installation

```bash
# Clone the repository
git clone https://github.com/<your-username>/lung-ct-hybrid-crnn.git
cd lung-ct-hybrid-crnn

# Create environment
conda create -n crnn python=3.10 -y
conda activate crnn

# Install dependencies
pip install torch torchvision
pip install numpy opencv-python-headless Pillow matplotlib scikit-learn scipy
pip install datasets huggingface_hub
```

### Requirements

| Package | Version |
|:--------|:--------|
| Python | â‰¥ 3.8 |
| PyTorch | â‰¥ 2.0 |
| torchvision | â‰¥ 0.15 |
| numpy | â‰¥ 1.24 |
| scikit-learn | â‰¥ 1.2 |
| opencv-python | â‰¥ 4.7 |
| datasets | â‰¥ 2.14 |
| huggingface_hub | â‰¥ 0.17 |
| matplotlib | â‰¥ 3.7 |
| scipy | â‰¥ 1.10 |

---

## Usage

### Run Full Experiment

```bash
python crnn_v2_experiment.py
```

This runs all eight phases sequentially:

1. **Phase 1** â€” Multi-seed Ã— multi-coreset training (9 runs Ã— 7 models)
2. **Phase 2** â€” Aggregated results with bootstrap CIs and statistical tests
3. **Phase 3** â€” Ablation study (backbone-only â†’ +BiLSTM â†’ +Attention â†’ full)
4. **Phase 4** â€” Fusion weight sensitivity sweep (Î± âˆˆ {0.5, 0.6, 0.7, 0.8, 0.9})
5. **Phase 5** â€” Coreset strategy comparison (Random vs. Stratified vs. FPS)
6. **Phase 6** â€” Figure generation (learning curves, confusion matrices, ROC/PR, reliability diagrams)
7. **Phase 7** â€” Per-class analysis and random baseline comparison
8. **Phase 8** â€” Calibration stability analysis across seeds

### Runtime Estimates

| Hardware | Estimated Time |
|:---------|:--------------:|
| CPU only | ~70â€“90 min |
| Single GPU (e.g., T4) | ~20â€“30 min |

### Google Colab

The script automatically detects Colab environments and uses `/content/drive/MyDrive/Maheswari/crnn_workspace_v2` as the output directory. Mount Google Drive before running:

```python
from google.colab import drive
drive.mount('/content/drive')
```

---

## Training Protocol

All models (proposed and baselines) are trained under a **unified protocol** to ensure fair comparison:

| Parameter | Baselines | Enhanced CRNN |
|:----------|:---------:|:-------------:|
| Max Epochs | 8 | 8 |
| Early Stopping Patience | 3 | 3 |
| Optimizer | AdamW | AdamW |
| Learning Rate | 3e-4 (flat) | 3e-4 head / 9e-5 backbone |
| Weight Decay | 1e-4 | 1e-4 |
| Scheduler | Cosine decay | Warmup (2 ep) + Cosine |
| Label Smoothing | 0.0 | 0.05 |
| Mixup | No | Î± = 0.1, p = 0.5 |
| Gradient Clipping | 1.0 | 1.0 |

---

## Preprocessing Pipeline

1. **Resize** to 224 Ã— 224
2. **CLAHE** (clip limit = 2.0, tile = 8Ã—8) for contrast enhancement
3. **Otsu thresholding** + morphological open/close for lung mask extraction
4. **Connected-component analysis** to isolate the largest lung region
5. **Background normalization** to mean background intensity
6. **ImageNet normalization** (Î¼ = [0.485, 0.456, 0.406], Ïƒ = [0.229, 0.224, 0.225])

Training augmentations include random resized crop, horizontal/vertical flips, rotation (Â±15Â°), color jitter, and affine translation.

---

## Evaluation Metrics

The framework reports a comprehensive set of metrics per model:

- **Classification:** Accuracy, Balanced Accuracy, Macro/Weighted F1, Macro Precision/Recall, Cohen's Îº, Matthews Correlation Coefficient
- **Ranking:** Macro AUC-ROC (OvR), Macro AUPRC, Log Loss
- **Calibration:** ECE (15 equal-width bins), Adaptive ECE (15 equal-mass bins), Brier Score, Maximum Calibration Error, Temperature Scaling
- **Statistical:** Bootstrap 95% CI, Wilcoxon signed-rank test (or paired t-test for n < 5), Cohen's d effect size

---

## Reproducibility

Full reproducibility is ensured through:

- **Deterministic seeding** â€” All random seeds (Python, NumPy, PyTorch, CUDA) are set before each run
- **Logged coreset indices** â€” Exact training sample indices saved per run as JSON
- **Hyperparameter table** â€” Complete configuration exported to `hyperparameters.json`
- **Environment logging** â€” PyTorch version, hardware specs, and GPU memory recorded in `environment.json`
- **Multi-seed protocol** â€” Results aggregated over 9 independent runs (3 experiment seeds Ã— 3 coreset seeds)

---

## Citation

If you find this work useful, please cite:

```bibtex
@article{maheswari2025learning,
  title={Learning under Extreme Data Scarcity: An Enhanced Hybrid CRNN with Calibration and Test-Time Augmentation for Multi-Class Lung CT Classification},
  author={Maheswari, V and Parveen Sultana, H},
  journal={<Journal>},
  year={2025},
  institution={School of Computer Science and Engineering, Vellore Institute of Technology}
}
```

---

## Acknowledgements

- Dataset provided by [`dorsar/lung-cancer`](https://huggingface.co/datasets/dorsar/lung-cancer) on Hugging Face
- Backbone pretrained weights from [TorchVision](https://pytorch.org/vision/stable/models.html) (ImageNet-1K)
- School of Computer Science and Engineering, Vellore Institute of Technology, Vellore

---

## License

This project is released under the [MIT License](LICENSE).
